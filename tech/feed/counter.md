## 微博计数
### MySQL 存储
```sql
select repost_count, comment_count, praise_count, view_count from weibo where weibo_id = ?
```

微博用户量和发布的微博量增加迅猛，计数存储数据量级也飞速增长，而 MySQL 数据库单表的存储量级达到几千万的时候，性能上就会有损耗。所以我们考虑使用分库分表的方式分散数据量，提升读取计数的性能

用 weibo_id 作为分区键，在选择分库分表的方式时，考虑以下两种：
1. 选择一种哈希算法对 weibo_id 计算哈希值，然后依据这个哈希值计算出需要存储到哪一个库哪一张表中
2. 按照 weibo_id 生成的时间来做分库分表，在分库分表的时候，可以先依据发号器的算法反解出时间戳，然后按照时间戳来做分库分表

按照时间来分库分表会造成数据访问的不均匀

### Redis 存储
计数的访问量级也迅速增加，数据库已经不能承担如此高的并发量了。于是考虑使用 Redis 来加速读请求，通过部署多个从节点来提升可用性和性能，并且通过 Hash 的方式对数据做分片，也基本上可以保证计数的读取性能。然而，这种数据库 + 缓存的方式有一个弊端：无法保证数据的一致性。所以，我们完全抛弃 MySQL，全面使用 Redis 来作为计数的存储组件

除了考虑计数的读取性能之外，由于热门微博的计数变化频率相当快，也需要考虑如何提升计数的写入性能。比如，每次在转发一条微博的时候，都需要增加这条微博的转发数，那么如果是热门微博，瞬时就可能会产生几万甚至几十万的转发

我们可以在转发微博的时候向消息队列写入一条消息，然后在消息处理程序中给这条微博的转发计数加 1。我们可以通过批量处理消息的方式进一步减小 Redis 的写压力


降低计数系统的存储成本
对原生 Redis 做一些改造，采用新的数据结构和数据类型来存储计数数据
1. 原生的 Redis 在存储 Key 时按照字符串类型来存储，如果我们使用 Long 类型来存储就只需要 8 个字节
2. 去除原生 Redis 中多余的指针

使用一个大的数组来存储计数信息
插入时：
```
h1 = hash1(weibo_id)
h2 = hash2(weibo_id) 

for s in 0, 1000
    pos = (h1 + h2 * s) % tsize
    if (isempty(pos) || isdelete(pos))
        t[pos] = item
```
查询时：
```
for s in 0, 1000
    pos = (h1 + h2 * s) % tsize
    if (!isempty(pos) && t[pos]==weibo_id)
        return t[pos]
return 0
```
删除时：
```
insert(FFFF)
```

微博的计数有转发数、评论数、浏览数、点赞数等等，如果每一个计数都需要存储 weibo_id，那么总共就需要 8（weibo_id）* 4（4 个微博 ID）+ 4（转发数）+ 4（评论数）+ 4（点赞数）+ 4（浏览数）= 48 字节。可以把相同微博 ID 的计数存储在一起，这样就只需要记录一个微博 ID


微博计数的数据具有明显的热点属性：越是最近的微博越是会被访问到，时间上久远的微博被访问的几率很小。为了尽量减少服务器的使用，考虑给计数服务增加 SSD 磁盘，然后将时间上比较久远的数据 dump 到磁盘上，内存中只保留最近的数据。当要读取冷数据的时候，使用单独的 I/O 线程异步地将冷数据从 SSD 磁盘中加载到一块儿单独的 Cold Cache 中


## 非系统未读数
当有人 @、评论、点赞或者私信的时候，会收到相应的未读提醒

使用通用的计数系统
在计数系统中增加一块内存区域，以用户 ID 为 Key 存储多个未读数，当有人评论时，增加未读评论的计数。当点击了未读数字进入通知页面，查看评论消息时，重置这些未读计数为零


## 系统通知未读数
如果采用通用的计数系统，当系统发送一个新的通知时，需要循环给每一个用户的未读数加 1。这样做存在两个问题：

首先，获取全量用户比较耗时，需要做一次全表的扫描，这不仅会对数据库造成很大的压力，而且查询全量用户数据的响应时间是很长的，对于在线业务来说是难以接受的。如果用户库已经做了分库分表，那么就要扫描所有的库表，响应时间就更长了。不过有一个折中的方法，那就是在发送系统通知之前，先从线下的数据仓库中获取全量的用户 ID，并且存储在一个本地的文件中，然后再轮询所有的用户 ID，给这些用户增加未读计数。然而它给所有人增加未读计数，会消耗非常长的时间。假如系统中有一个亿的用户，给一个用户增加未读数需要消耗 1ms，那么给所有人都增加未读计数就需要 100000000 * 1 / 1000 = 100000 秒，也就是超过一天的时间；即使启动 100 个线程并发处理，也需要十几分钟

另外，使用这种方式需要给系统中的每一个用户都记一个未读数的值，而在系统中，活跃用户只是很少的一部分，大部分的用户是不活跃的，甚至从来没有打开过系统通知，为这些用户记录未读数显然是一种浪费


系统通知实际上是存储在一个大的列表中的，这个列表对所有用户共享，也就是所有人看到的都是同一份系统通知的数据。不过不同的人最近看到的消息不同，所以每个人会有不同的未读数。因此，可以记录一下在这个列表中每个人看过最后一条消息的 ID，然后统计这个 ID 之后有多少条消息，就得到未读数了

1. 用户访问系统通知页面设置未读数为 0，将用户最近看过的通知 ID 设置为最新的一条系统通知 ID
2. 如果最近看过的通知 ID 为空，则认为是一个新的用户，返回未读数为 0
3. 对于非活跃用户，比如最近一个月都没有登录和使用过系统的用户，可以把用户最近看过的通知 ID 清空，节省内存空间


## 全量用户打点
如果逐个通知用户，延迟是无法接受的。因此可以采用和系统通知类似的方案

首先，为每一个用户存储一个时间戳，代表最近点过这个红点的时间，用户点了红点，就把这个时间戳设置为当前时间

然后，记录一个全局的时间戳，这个时间戳标识最新的一次打点时间，如果在后台操作给全体用户打点，就更新这个时间戳为当前时间。在判断是否需要展示红点时，只需要判断用户的时间戳和全局时间戳的大小，如果用户时间戳小于全局时间戳，代表在用户最后一次点击红点之后又有新的红点推送，那么就要展示红点，反之，就不展示红点了


## 信息流未读数
信息流的未读数之所以复杂主要有以下原因：

首先，微博的信息流是基于关注关系的，未读数也是基于关注关系的，就是说，你关注的人发布了新的微博，那么你作为粉丝未读数就要增加 1。如果微博用户都只有几百粉丝就比较简单，发微博的时候系统给所有粉丝的未读数增加 1 就可以。但是对于一些几千万甚至上亿粉丝的微博大 V 就比较麻烦，增加未读数可能需要几个小时。这样会导致，博文发出几小时之后才收到提醒，这显然是不能接受的。所以必须考虑未读数的延迟

其次，信息流未读数请求量极大、并发极高，这是因为接口是客户端轮询请求的，不是用户触发的。作为微博的非核心接口，不太可能使用大量的机器来抗未读数请求，因此，如何使用有限的资源来支撑如此高的流量是这个方案的难点

最后，它不像系统通知那样有共享的存储，因为每个人关注的人不同，信息流的列表也就不同，所以也就没办法采用系统通知未读数的方案


具体解决方案如下：
首先，在通用计数器中记录每一个用户发布的博文数

然后，在 Redis 中记录一个人所有关注人的博文数快照，当用户点击未读消息重置未读数为 0 时，将他关注所有人的博文数刷新到快照中。这样，他关注所有人的博文总数减去快照中的博文总数就是他的信息流未读数


假如用户 A，关注了用户 B、C、D，其中 B 发布的博文数是 10，C 发布的博文数是 8，D 发布的博文数是 14，而在用户 A 最近一次查看未读消息时，记录在快照中的这三个用户的博文数分别是 6、7、12，因此用户 A 的未读数就是(10 - 6) + (8 - 7) + (14 - 12) = 7

这个方案设计简单，并且是全内存操作，性能足够好，能够支撑比较高的并发

这个方案也有一些缺陷，比如说快照中需要存储关注关系，如果关注关系变更的时候更新不及时，那么就会造成未读数不准确；快照采用的是全缓存存储，如果缓存满了就会剔除一些数据，那么被剔除用户的未读数就变为 0 了。但是好在用户对于未读数的准确度要求不高，因此，这些缺陷也是可以接受的
